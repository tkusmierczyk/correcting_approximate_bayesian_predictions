{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horseshoe Priors: Sparse Models with Failure of Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use Bayesian regression with horseshoe priors to model [corn data](https://core.ac.uk/download/pdf/397803.pdf). The model is implemented using the publicly available [Stan code](https://github.com/yao-yl/Evaluating-Variational-Inference/blob/master/Rcode/glmbernoullirhs.stan) (additional details can be found in our paper). \n",
    "For this model the quality of the variational approximation is sensitive to random initialization and the stochastic variation during the optimization, so that occasionally the posterior is reasonable whereas for some runs it converges toa very bad solution. We fix it with help of a decision-maker (neural network) showing that it is capable of correcting errors in posterior approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pystan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux import tonumpy, parse_script_args, dict2str, print2\n",
    "import losses\n",
    "import horseshoe_regression\n",
    "import regression_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    env = torch.cuda\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    env = torch\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "    \n",
    "def totorch(array):\n",
    "    return torch.tensor(array, dtype=env.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing: <-f>\n"
     ]
    }
   ],
   "source": [
    "args = parse_script_args() # arguments can be passed in the format of NAME1=FLOATVAL1,NAME2=[STRVAL2],NAME3=INTVAL3,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization general parmeters\n",
    "SEED = args.get(\"SEED\", 5)\n",
    "VI_NITER = int(args.get(\"VI_NITER\", 1e6))\n",
    "HMC_NITER = int(args.get(\"HMC_NITER\", 1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected loss: tilted/squared/exptilted/expsquared\n",
    "LOSS = args.get(\"LOSS\", \"tilted\")\n",
    "TILTED_Q = args.get(\"TILTED_Q\", 0.5) # relevant only for tilted and exptilted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization\n",
    "LAMBDA = args.get(\"LAMBDA\", 1)\n",
    "REGULARIZATION = args.get(\"REGULARIZATION\", \"boot1\").lower() # const/boot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_NO = args.get(\"OUTPUT_NO\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIGURATION SUMMARY: HMC_NITER=0 LAMBDA=1 LOSS=tilted OUTPUT_NO=3 REGULARIZATION=boot1 SEED=5 TILTED_Q=0.5 VI_NITER=1000000\n"
     ]
    }
   ],
   "source": [
    "print(\"CONFIGURATION SUMMARY: %s\" % dict2str(globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f28fbffe550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LossFactory] Configuration: TILTED_Q=0.5 LINEX_C=None\n",
      "> <tilted> loss: tilted_loss_fixedq with (analytical/Bayes estimator) h: tilted_optimal_h_fixedq\n"
     ]
    }
   ],
   "source": [
    "loss, optimal_h_bayes_estimator = losses.LossFactory(**globals()).create(LOSS)\n",
    "print2(\"> <%s> loss: %s with (analytical/Bayes estimator) h: %s\" % \n",
    "        (LOSS, loss.__name__, optimal_h_bayes_estimator.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "empirical_risk = lambda preds, y: loss(preds, totorch(y)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, = regression_data.load_corn()\n",
    "Y = Y[:, OUTPUT_NO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 700), (40, 700))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_MASK = np.array([i%2!=0 for i in range(X.shape[0])], dtype=bool)\n",
    "X_TRAIN, Y_TRAIN = X[TRAIN_MASK,:], Y[TRAIN_MASK]\n",
    "X_TEST, Y_TEST = X[~TRAIN_MASK,:], Y[~TRAIN_MASK]\n",
    "\n",
    "X_TRAIN.shape, X_TEST.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from horseshoe_regression_model.pkl\n",
      "Failed. Recompiling model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_ac3e4982d09648ff2d5c2db32402b71b NOW.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation time: 68.19340753555298\n"
     ]
    }
   ],
   "source": [
    "pickle_path = \"horseshoe_regression_model.pkl\"\n",
    "try:\n",
    "    print(\"Loading model from %s\" % pickle_path)\n",
    "    sm = pickle.load(open(pickle_path, \"rb\"))\n",
    "except:\n",
    "    print(\"Failed. Recompiling model\")\n",
    "    start = time.time()\n",
    "    sm = pystan.StanModel(model_code=horseshoe_regression.CODE)\n",
    "    print(\"Compilation time:\", time.time()-start)\n",
    "    pickle.dump(sm, open(pickle_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_data = horseshoe_regression.create_data(X_TRAIN, Y_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VI fit from horseshoe_regression_vi_fit_5_1000000.pkl\n",
      "Failed. Running new inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Automatic Differentiation Variational Inference (ADVI) is an EXPERIMENTAL ALGORITHM.\n",
      "WARNING:pystan:ADVI samples may be found on the filesystem in the file `/tmp/tmpotzaafzl/output.csv`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VI time: 887.9219462871552\n"
     ]
    }
   ],
   "source": [
    "path = \"horseshoe_regression_vi_fit_%s_%s.pkl\" % (SEED,VI_NITER)\n",
    "\n",
    "try:\n",
    "    print(\"Loading VI fit from %s\" % path)\n",
    "    vi_fit = pickle.load(open(path, \"rb\"))\n",
    "except:\n",
    "    print(\"Failed. Running new inference.\")\n",
    "    start = time.time()\n",
    "    vi_fit = sm.vb(data=stan_data, iter=VI_NITER, output_samples=1e3,\n",
    "                   tol_rel_obj=0.001, eta=0.1, seed=SEED, verbose=True)\n",
    "    print(\"VI time:\", time.time()-start)\n",
    "    pickle.dump(vi_fit, open(path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PSIS-LOO Evaluation\n",
    "failed_ks = 0.0\n",
    "# import psis\n",
    "# paramname2ix = dict((paramname, ix) for ix, paramname in enumerate(vi_fit[\"sampler_param_names\"]))\n",
    "# s = lambda paramname: vi_fit[\"sampler_params\"][paramname2ix[paramname]] # access samples          \n",
    "# loglik = np.array([s(\"loglik[%i]\" % i) for i in range(1, X_TRAIN.shape[0]+1)]).T\n",
    "\n",
    "# loo, loos, ks = psis.psisloo(loglik)\n",
    "# failed_ks = sum(ks > 0.7) / len(ks)\n",
    "# print(\"Fraction of failed k: %.3f\" % failed_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_train = totorch(horseshoe_regression.sample_predictive_y0_vi(vi_fit, X_TRAIN))\n",
    "ys_test = totorch(horseshoe_regression.sample_predictive_y0_vi(vi_fit, X_TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hstar_train = optimal_h_bayes_estimator(ys_train)\n",
    "hstar_test = optimal_h_bayes_estimator(ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VI optimal risk: train:0.327 test:0.3408\n"
     ]
    }
   ],
   "source": [
    "vi_train_risk = empirical_risk(hstar_train, Y_TRAIN).item()\n",
    "vi_test_risk = empirical_risk(hstar_test, Y_TEST).item()\n",
    "print(\"VI optimal risk: train:%.3f test:%.4f\" % (vi_train_risk, vi_test_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMC Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping HMC evaluation\n",
      "HMC optimal risk: train:0.0000 test:0.0000\n"
     ]
    }
   ],
   "source": [
    "if HMC_NITER<=0:\n",
    "    \n",
    "    print(\"Skipping HMC evaluation\")\n",
    "    hmc_train_risk, hmc_test_risk = 0.0, 0.0\n",
    "    \n",
    "else:\n",
    "    \n",
    "    hmc_fit = sm.sampling(data=stan_data, iter=HMC_NITER, n_jobs=1)\n",
    "\n",
    "    ys_train_hmc = totorch(horseshoe_regression.sample_predictive_y0_hmc(hmc_fit, X_TRAIN))\n",
    "    ys_test_hmc = totorch(horseshoe_regression.sample_predictive_y0_hmc(hmc_fit, X_TEST))\n",
    "\n",
    "    hstar_train_hmc = optimal_h_bayes_estimator(ys_train_hmc)\n",
    "    hstar_test_hmc = optimal_h_bayes_estimator(ys_test_hmc)\n",
    "\n",
    "    hmc_train_risk = empirical_risk(hstar_train_hmc, Y_TRAIN).item()\n",
    "    hmc_test_risk = empirical_risk(hstar_test_hmc, Y_TEST).item()\n",
    "\n",
    "print(\"HMC optimal risk: train:%.4f test:%.4f\" % (hmc_train_risk, hmc_test_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[regularization] Using regularization: boot1 with lambda=1\n",
      "[regularization] Bootstrap-based regularization\n",
      "[get_bootstrap_decisions] 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Automatic Differentiation Variational Inference (ADVI) is an EXPERIMENTAL ALGORITHM.\n",
      "WARNING:pystan:ADVI samples may be found on the filesystem in the file `/tmp/tmp5wirwn8u/output.csv`\n"
     ]
    }
   ],
   "source": [
    "import regression_regularization as regularization\n",
    "\n",
    "HSTAR_TRAIN = hstar_train # default regularization mean\n",
    "\n",
    "print(\"[regularization] Using regularization: %s with lambda=%s\" % (REGULARIZATION, LAMBDA))\n",
    "if REGULARIZATION.startswith(\"boot\"):     \n",
    "    print(\"[regularization] Bootstrap-based regularization\")    \n",
    "    stan_data_generator = horseshoe_regression.yield_data_bootstrap(X_TRAIN, Y_TRAIN)    \n",
    "    sample_predictive_y0_train = lambda vi_fit: horseshoe_regression.sample_predictive_y0_vi(vi_fit, X_TRAIN)\n",
    "    sample_predictive_y0_test = lambda vi_fit: horseshoe_regression.sample_predictive_y0_vi(vi_fit, X_TEST)\n",
    "    optimal_h_bayes_estimator_np = lambda ys: tonumpy(optimal_h_bayes_estimator(totorch(ys)))\n",
    "\n",
    "    hstar1_train, _ = regularization.get_bootstrap_decisions(sm, stan_data_generator, \n",
    "                                        sample_predictive_y0_train, sample_predictive_y0_test, \n",
    "                                        optimal_h_bayes_estimator_np, num_repetitions=5, iter=VI_NITER)        \n",
    "    LAMBDA_TRAIN = LAMBDA * 1.0/(2. * hstar1_train.std(0)**2)\n",
    "        \n",
    "    if REGULARIZATION.startswith(\"boot1\"):\n",
    "        print(\"[regularization] Bootstrap-based regularization: overwritting regularization mean\")\n",
    "        HSTAR_TRAIN = totorch( hstar1_train.mean(0) )\n",
    "        \n",
    "elif REGULARIZATION.startswith(\"const\"): \n",
    "    LAMBDA_TRAIN = regularization.get_regularization_constant(ys_train, LAMBDA)\n",
    "elif REGULARIZATION.startswith(\"std\"): \n",
    "    LAMBDA_TRAIN = regularization.get_regularization_std(ys_train, LAMBDA)\n",
    "elif REGULARIZATION.startswith(\"qdiff\"): \n",
    "    LAMBDA_TRAIN = regularization.get_regularization_qdiff(ys_train, LAMBDA)\n",
    "else:    \n",
    "    raise Exception(\"[regularization] Unknown regularization method name: %s\" % method)    \n",
    "\n",
    "LAMBDA_TRAIN = totorch(LAMBDA_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_empirical_risk(q):    \n",
    "    q = max(0., min(1., q[0]))\n",
    "    h = losses.tilted_optimal_h(ys_train, q) # obtaing a quantile in a very convoluted way :)\n",
    "    risk = empirical_risk(h, Y_TRAIN).item() \n",
    "    regularizer = ( LAMBDA_TRAIN * (h-HSTAR_TRAIN)**2 ).mean()\n",
    "    obj = risk + regularizer\n",
    "    print(\"evaluating training risk @ q=%.2f => risk=%.3f => obj=%.3f\" % (q, risk, obj))\n",
    "    return obj                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "res = minimize(training_empirical_risk, [TILTED_Q], \n",
    "               method='Nelder-Mead', options={'xtol': 1e-5, 'disp': True, \"maxiter\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = max(0., min(1., res[\"x\"][0]))\n",
    "q_train_risk = empirical_risk(losses.tilted_optimal_h(ys_train, q), Y_TRAIN).item() \n",
    "q_test_risk = empirical_risk(losses.tilted_optimal_h(ys_test, q), Y_TEST).item() \n",
    "print(\"Quantile optimal risk: train:%.3f test:%.4f\" % (q_train_risk, q_test_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features \n",
    "Quantiles = np.array([np.percentile(ys_train, int(q*100), axis=0) for q in np.arange(0., 1.01, 0.05)])\n",
    "X_train = Quantiles\n",
    "\n",
    "Quantiles = np.array([np.percentile(ys_test, int(q*100), axis=0) for q in np.arange(0., 1.01, 0.05)])\n",
    "X_test = Quantiles\n",
    "\n",
    "X_train, X_test = totorch(X_train.T), totorch(X_test.T)\n",
    "X_test.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "NUM_RESTARTS = 10\n",
    "best_decision_maker, best_risk = None, float(\"inf\") \n",
    "for _ in range(NUM_RESTARTS):\n",
    "    print(\"RESTARTING\")\n",
    "    \n",
    "    decision_maker = nn.Sequential(\n",
    "      nn.Linear(X_train.shape[1], 20),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(20, 10),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(10, 1)\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(decision_maker.parameters(), lr=0.01)\n",
    "    start = time.time()\n",
    "    for i in range(20000):\n",
    "        h = decision_maker(X_train).view(-1)\n",
    "        train_loss = empirical_risk(h, Y_TRAIN)  \n",
    "        regularizer = ( LAMBDA_TRAIN * (h-HSTAR_TRAIN)**2 ).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (train_loss+regularizer).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%1000==0:\n",
    "            print(\"[%.2fs] %i. iteration: training batch risk: %.3f regularizer: %.3f train risk: %.3f test risk: %.3f\" % \n",
    "              (time.time()-start, i, train_loss.item(), regularizer.item(),\n",
    "               empirical_risk(decision_maker(X_train).view(-1), Y_TRAIN).item(),\n",
    "               empirical_risk(decision_maker(X_test).view(-1), Y_TEST).item()))\n",
    "            \n",
    "    dm_train_risk = empirical_risk(decision_maker(X_train).view(-1), Y_TRAIN).item()           \n",
    "    if dm_train_risk<best_risk: best_decision_maker, best_risk = decision_maker, dm_train_risk\n",
    "decision_maker = best_decision_maker                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_train_risk = empirical_risk(decision_maker(X_train).view(-1), Y_TRAIN).item()\n",
    "dm_test_risk = empirical_risk(decision_maker(X_test).view(-1), Y_TEST).item()\n",
    "print(\"DM optimal risk: train:%.3f test:%.4f\" % (dm_train_risk, dm_test_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [[SEED, LOSS, TILTED_Q, REGULARIZATION, LAMBDA, VI_NITER, HMC_NITER, OUTPUT_NO,\n",
    "            vi_train_risk, vi_test_risk, \n",
    "            q_train_risk, q_test_risk, \n",
    "            dm_train_risk, dm_test_risk,\n",
    "            hmc_train_risk, hmc_test_risk, \n",
    "            failed_ks]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"horseshore_regression_%i_%s_%s_%s_%s_%s_%s.csv\" % \\\n",
    "            (SEED, LOSS, TILTED_Q, REGULARIZATION, LAMBDA, VI_NITER, OUTPUT_NO)\n",
    "print(\"Saving to %s\" % path)\n",
    "pd.DataFrame(results).to_csv(path, header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
