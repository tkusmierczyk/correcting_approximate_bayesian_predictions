{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXY51qRDhBKi"
   },
   "source": [
    "# *Matrix Factorization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we model last.fm data with Probabilistic Matrix Factorization model (implemented in pyTorch). We compare performance of a traditional approach against a decision-maker (neural network) to show that it is capable of correcting errors due to posterior approximation. The inference is carried out using automatic differentiation variational inference, and we split the data randomly into equally sized training and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XovyapF640zV"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from torch.nn.functional import softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vtjkOrxIg4-O",
    "outputId": "8eab1b5a-e9f0-42af-a811-ae77658239c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    env = torch.cuda\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    env = torch\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "    \n",
    "def totorch(array):\n",
    "    return torch.tensor(array, dtype=env.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aux import print2, tonumpy, flatten_first_two_dims, parse_script_args, dict2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import losses, aux_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import count_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing: <-f>\n"
     ]
    }
   ],
   "source": [
    "args = parse_script_args() # arguments can be passed in the format of NAME1=FLOATVAL1,NAME2=[STRVAL2],NAME3=INTVAL3,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization general parmeters\n",
    "SEED = args.get(\"SEED\", 1)\n",
    "NITER  = 30001 # number of iterations - around 70k is the right number for mininbatch=10\n",
    "LR = 0.01 # 0.1 is the right number for the full batch, 0.001 is advised for mininbatch=10\n",
    "MINIBATCH = 100 # how many rows of the matrix per minibatch\n",
    "\n",
    "# model parameter\n",
    "K = 20 # number of latent variables\n",
    "\n",
    "# number of samples used to approximate ELBO term\n",
    "NSAMPLES = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected loss: tilted/squared/exptilted/expsquared\n",
    "LOSS = args.get(\"LOSS\", \"tilted\")\n",
    "TILTED_Q = args.get(\"TILTED_Q\", 0.5) # relevant only for tilted and exptilted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = args.get(\"S\", 1000)\n",
    "NUM_QUANTILES = args.get(\"NUM_QUANTILES\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization: lambda=0 means no regularization\n",
    "REGULARIZATION = args.get(\"REGULARIZATION\", \"const\").lower()\n",
    "LAMBDA = args.get(\"LAMBDA\", 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIGURATION SUMMARY: K=20 LAMBDA=0.0 LOSS=tilted LR=0.01 MINIBATCH=100 NITER=30001 NSAMPLES=11 NUM_QUANTILES=20 REGULARIZATION=const S=1000 SEED=1 TILTED_Q=0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"CONFIGURATION SUMMARY: %s\" % dict2str(globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIOoOPjF5mnS"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed (~true) data: sparsity level=0.565\n",
      "Fixed (~true) data: mean=1.202, std=1.762\n"
     ]
    }
   ],
   "source": [
    "Y, MASK = count_data.lastfm_data(log=True)\n",
    "\n",
    "print(\"Fixed (~true) data: sparsity level=%.3f\" % ((Y==0).sum() / (Y.shape[0]*Y.shape[1])) )\n",
    "print(\"Fixed (~true) data: mean=%.3f, std=%.3f\" % (Y.mean(), Y.std()))\n",
    "\n",
    "Y_TRAIN, Y_TEST, TRAIN_MASK, TEST_MASK = count_data.test_train_split(Y, MASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LossFactory] Configuration: TILTED_Q=0.5 LINEX_C=None\n",
      "> <tilted> loss: tilted_loss_fixedq with (analytical/Bayes estimator) h: tilted_optimal_h_fixedq\n"
     ]
    }
   ],
   "source": [
    "loss, optimal_h_bayes_estimator = losses.LossFactory(**globals()).create(LOSS)\n",
    "print2(\"> <%s> loss: %s with (analytical/Bayes estimator) h: %s\" % \n",
    "        (LOSS, loss.__name__, optimal_h_bayes_estimator.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_risk1(preds_flat, y, mask):\n",
    "  \"\"\"\n",
    "    preds_flat  torch tensor (vector)\n",
    "    y, mask     numpy array\n",
    "  \"\"\"\n",
    "  assert len(preds_flat.shape)==1\n",
    "  assert y.shape==mask.shape\n",
    "  assert preds_flat.shape[0]==y.shape[0]*y.shape[1]\n",
    "  mask_flat = torch.tensor(mask, dtype=env.uint8).view(-1)\n",
    "  test_preditions = torch.masked_select(preds_flat, mask_flat)\n",
    "  test_y = totorch( y[mask.astype(bool)] )  \n",
    "  return loss(test_preditions, test_y).mean()\n",
    "  \n",
    "\n",
    "def empirical_risk(preds, y, mask):\n",
    "  assert preds.shape==y.shape\n",
    "  assert y.shape==mask.shape  \n",
    "  preds_flat = preds.view(-1)\n",
    "  return empirical_risk1(preds_flat, y, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_A7LUjFe5oRV"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import normal_normal_mf as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model from PMF_qwqz_1.pkl\n",
      "Failed. Recomputing.\n",
      "[0.05s] 0. iteration, 0. epoch\n",
      "[0.08s] 1. iteration, 0. epoch\n",
      "[0.12s] 2. iteration, 0. epoch\n",
      "[0.15s] 3. iteration, 0. epoch\n",
      "[0.16s] 4. iteration, 0. epoch\n",
      "[0.20s] 5. iteration, 0. epoch\n",
      "[0.25s] 6. iteration, 0. epoch\n",
      "[0.29s] 7. iteration, 0. epoch\n",
      "[0.32s] 8. iteration, 0. epoch\n",
      "[0.36s] 9. iteration, 0. epoch\n",
      "[12.34s] 1000. iteration, 100. epoch\n",
      "[24.42s] 2000. iteration, 200. epoch\n",
      "[36.50s] 3000. iteration, 300. epoch\n",
      "[49.31s] 4000. iteration, 400. epoch\n",
      "[70.45s] 5000. iteration, 500. epoch\n",
      "[107.72s] 6000. iteration, 600. epoch\n",
      "[134.29s] 7000. iteration, 700. epoch\n",
      "[148.24s] 8000. iteration, 800. epoch\n",
      "[161.92s] 9000. iteration, 900. epoch\n",
      "[180.39s] 10000. iteration, 1000. epoch\n",
      "[199.64s] 11000. iteration, 1100. epoch\n",
      "[221.72s] 12000. iteration, 1200. epoch\n",
      "[260.53s] 13000. iteration, 1300. epoch\n",
      "[282.62s] 14000. iteration, 1400. epoch\n",
      "[293.96s] 15000. iteration, 1500. epoch\n",
      "[305.87s] 16000. iteration, 1600. epoch\n",
      "[316.96s] 17000. iteration, 1700. epoch\n",
      "[329.62s] 18000. iteration, 1800. epoch\n",
      "[340.83s] 19000. iteration, 1900. epoch\n",
      "[353.46s] 20000. iteration, 2000. epoch\n",
      "[369.90s] 21000. iteration, 2100. epoch\n",
      "[383.12s] 22000. iteration, 2200. epoch\n",
      "[404.96s] 23000. iteration, 2300. epoch\n",
      "[428.64s] 24000. iteration, 2400. epoch\n",
      "[455.56s] 25000. iteration, 2500. epoch\n",
      "[477.75s] 26000. iteration, 2600. epoch\n",
      "[500.89s] 27000. iteration, 2700. epoch\n",
      "[537.21s] 28000. iteration, 2800. epoch\n",
      "[565.02s] 29000. iteration, 2900. epoch\n",
      "[588.33s] 30000. iteration, 3000. epoch\n"
     ]
    }
   ],
   "source": [
    "path = \"PMF_qwqz_%s.pkl\" % SEED\n",
    "try:\n",
    "    print(\"Loading the model from %s\" % path)\n",
    "    qw, qz = pickle.load(open(path, \"rb\"))\n",
    "except:\n",
    "    print(\"Failed. Recomputing.\")\n",
    "    qw, qz = model.vi_inference(Y, TRAIN_MASK, K, MINIBATCH, NSAMPLES, SEED, NITER, LR)\n",
    "    pickle.dump((qw,qz), open(path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from normal_normal_mf import sample_predictive_y\n",
    "ys = sample_predictive_y(qw, qz, nsamples_theta=1000, nsamples_y=1)        \n",
    "#pickle.dump(ys, open(\"PMF_samples_%i.pkl\" % SEED, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ys = pickle.load(open(\"PMF_samples_%i.pkl\" % SEED, \"rb\"))\n",
    "Y_SAMPLES = tonumpy(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvi = optimal_h_bayes_estimator(ys) # approximately optimal decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal risk: train:0.310 test:0.4620\n"
     ]
    }
   ],
   "source": [
    "vi_train_risk = empirical_risk(hvi, Y, TRAIN_MASK).item()\n",
    "vi_test_risk = empirical_risk(hvi, Y, TEST_MASK).item()\n",
    "print(\"optimal risk: train:%.3f test:%.4f\" % (vi_train_risk, vi_test_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regularization_constant(ys, l=LAMBDA):\n",
    "    return np.ones(ys.shape[1:])*l\n",
    "\n",
    "\n",
    "def get_regularization_std(ys, l=LAMBDA):\n",
    "    sigma = tonumpy(ys).std(0) * l\n",
    "    lam = 1. / sigma\n",
    "    return lam    \n",
    "\n",
    "\n",
    "def get_regularization_qdiff(ys, l=LAMBDA):\n",
    "    h = optimal_h_bayes_estimator(ys).clone().detach()\n",
    "    Quantiles = np.array([np.percentile(Y_SAMPLES, int(q*100), axis=0) for q in [0.5-abs(l), 0.5+abs(l)]])\n",
    "    sigma = abs(Quantiles[0,:,:]-Quantiles[1,:,:])\n",
    "    lam = 1. / sigma\n",
    "    return lam\n",
    "\n",
    "\n",
    "def get_regularization(ys, method=REGULARIZATION, l=LAMBDA):\n",
    "    if method.startswith(\"const\"): return get_regularization_constant(ys, l)\n",
    "    if method.startswith(\"std\"): return get_regularization_std(ys, l)\n",
    "    if method.startswith(\"qdiff\"): return get_regularization_qdiff(ys, l)\n",
    "    raise Exception(\"Unknown regularization method name: %s\" % method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSTAR = hvi\n",
    "LAMBDA_TRAIN = get_regularization(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating training risk @ q=0.50 => risk=0.310 => obj=0.310\n",
      "evaluating training risk @ q=0.53 => risk=0.315 => obj=0.315\n",
      "evaluating training risk @ q=0.47 => risk=0.309 => obj=0.309\n",
      "evaluating training risk @ q=0.45 => risk=0.314 => obj=0.314\n",
      "evaluating training risk @ q=0.45 => risk=0.314 => obj=0.314\n",
      "evaluating training risk @ q=0.49 => risk=0.309 => obj=0.309\n",
      "evaluating training risk @ q=0.50 => risk=0.310 => obj=0.310\n",
      "evaluating training risk @ q=0.48 => risk=0.309 => obj=0.309\n",
      "evaluating training risk @ q=0.49 => risk=0.309 => obj=0.309\n",
      "evaluating training risk @ q=0.48 => risk=0.309 => obj=0.309\n",
      "evaluating training risk @ q=0.48 => risk=0.309 => obj=0.309\n",
      "evaluating training risk @ q=0.49 => risk=0.309 => obj=0.309\n",
      "evaluating training risk @ q=0.49 => risk=0.309 => obj=0.309\n",
      "evaluating training risk @ q=0.49 => risk=0.309 => obj=0.309\n",
      "evaluating training risk @ q=0.49 => risk=0.309 => obj=0.309\n",
      "evaluating training risk @ q=0.49 => risk=0.309 => obj=0.309\n",
      "evaluating training risk @ q=0.49 => risk=0.309 => obj=0.309\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.308865\n",
      "         Iterations: 7\n",
      "         Function evaluations: 17\n"
     ]
    }
   ],
   "source": [
    "def training_empirical_risk(q):    \n",
    "    q = max(0., min(1., q[0]))\n",
    "    h = losses.tilted_optimal_h(ys, q) # obtaing a quantile in a very convoluted way :)\n",
    "    risk = empirical_risk(h, Y, TRAIN_MASK).item() \n",
    "    regularizer = ( LAMBDA_TRAIN[TRAIN_MASK.astype(bool)] * tonumpy((h-HSTAR)**2)[TRAIN_MASK.astype(bool)] ).mean()\n",
    "    obj = risk + regularizer\n",
    "    print(\"evaluating training risk @ q=%.2f => risk=%.3f => obj=%.3f\" % (q, risk, obj))\n",
    "    return obj            \n",
    "\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "res = minimize(training_empirical_risk, [0.5], \n",
    "               method='Nelder-Mead', options={'xtol': 1e-3, 'disp': True, \"maxiter\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal risk: train:0.309 test:0.4598\n"
     ]
    }
   ],
   "source": [
    "q = max(0., min(1., res[\"x\"][0]))\n",
    "h = losses.tilted_optimal_h(ys, q)\n",
    "q_train_risk = empirical_risk(h, Y, TRAIN_MASK).item() \n",
    "q_test_risk = empirical_risk(h, Y, TEST_MASK).item() \n",
    "print(\"optimal risk: train:%.3f test:%.4f\" % (q_train_risk, q_test_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6351401470>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 992, 100)\n"
     ]
    }
   ],
   "source": [
    "from normal_normal_mf import sample_predictive_y\n",
    "ys = sample_predictive_y(qw, qz, nsamples_theta=S, nsamples_y=1)        \n",
    "Y_SAMPLES = tonumpy(ys)\n",
    "print(Y_SAMPLES.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 992, 100)\n"
     ]
    }
   ],
   "source": [
    "# Prepare features \n",
    "step = 1.0/(NUM_QUANTILES+1)\n",
    "qs = list(np.arange(step, 0.99, step))\n",
    "Quantiles = np.array([np.percentile(Y_SAMPLES, int(q*100), axis=0) for q in qs])\n",
    "\n",
    "X = Quantiles\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decision_maker = nn.Sequential(\n",
    "  nn.Linear(X.shape[0], 20),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(20, 10),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(10, 1)\n",
    ")\n",
    "\n",
    "print(decision_maker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(decision_maker.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat = totorch( X.reshape(X.shape[0],-1).transpose() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial risk: train:0.712 test:0.707\n"
     ]
    }
   ],
   "source": [
    "predictions = decision_maker(X_flat).view(-1)\n",
    "print(\"initial risk: train:%.3f test:%.3f\" % (empirical_risk1(predictions, Y, TRAIN_MASK),\n",
    "                                                empirical_risk1(predictions, Y, TEST_MASK)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSTAR_FROZEN = HSTAR.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04s] 0. iteration, 0. epoch\n",
      "epoch: 0 training batch risk: 0.746 train risk: 0.632 test risk: 0.629\n",
      "[0.09s] 1. iteration, 0. epoch\n",
      "epoch: 0 training batch risk: 0.585 train risk: 0.572 test risk: 0.581\n",
      "[0.14s] 2. iteration, 0. epoch\n",
      "epoch: 0 training batch risk: 0.559 train risk: 0.503 test risk: 0.532\n",
      "[0.21s] 3. iteration, 0. epoch\n",
      "epoch: 0 training batch risk: 0.538 train risk: 0.442 test risk: 0.492\n",
      "[0.24s] 4. iteration, 0. epoch\n",
      "epoch: 0 training batch risk: 0.454 train risk: 0.383 test risk: 0.461\n",
      "[0.31s] 5. iteration, 0. epoch\n",
      "epoch: 0 training batch risk: 0.427 train risk: 0.333 test risk: 0.446\n",
      "[0.35s] 6. iteration, 0. epoch\n",
      "epoch: 0 training batch risk: 0.308 train risk: 0.312 test risk: 0.459\n",
      "[0.42s] 7. iteration, 0. epoch\n",
      "epoch: 0 training batch risk: 0.310 train risk: 0.337 test risk: 0.500\n",
      "[0.47s] 8. iteration, 0. epoch\n",
      "epoch: 0 training batch risk: 0.327 train risk: 0.364 test risk: 0.530\n",
      "[0.52s] 9. iteration, 0. epoch\n",
      "epoch: 0 training batch risk: 0.369 train risk: 0.362 test risk: 0.529\n",
      "[0.56s] 10. iteration, 1. epoch\n",
      "epoch: 1 training batch risk: 0.342 train risk: 0.339 test risk: 0.509\n",
      "[0.61s] 11. iteration, 1. epoch\n",
      "epoch: 1 training batch risk: 0.330 train risk: 0.307 test risk: 0.479\n",
      "[0.65s] 12. iteration, 1. epoch\n",
      "epoch: 1 training batch risk: 0.301 train risk: 0.282 test risk: 0.451\n",
      "[0.72s] 13. iteration, 1. epoch\n",
      "epoch: 1 training batch risk: 0.295 train risk: 0.270 test risk: 0.430\n",
      "[0.78s] 14. iteration, 1. epoch\n",
      "epoch: 1 training batch risk: 0.263 train risk: 0.272 test risk: 0.420\n",
      "[0.80s] 15. iteration, 1. epoch\n",
      "epoch: 1 training batch risk: 0.281 train risk: 0.278 test risk: 0.415\n",
      "[0.85s] 16. iteration, 1. epoch\n",
      "epoch: 1 training batch risk: 0.288 train risk: 0.282 test risk: 0.412\n",
      "[0.89s] 17. iteration, 1. epoch\n",
      "epoch: 1 training batch risk: 0.264 train risk: 0.282 test risk: 0.411\n",
      "[0.93s] 18. iteration, 1. epoch\n",
      "epoch: 1 training batch risk: 0.294 train risk: 0.281 test risk: 0.411\n",
      "[0.96s] 19. iteration, 1. epoch\n",
      "epoch: 1 training batch risk: 0.285 train risk: 0.274 test risk: 0.410\n",
      "[1.01s] 20. iteration, 2. epoch\n",
      "epoch: 2 training batch risk: 0.268 train risk: 0.264 test risk: 0.408\n",
      "[1.05s] 21. iteration, 2. epoch\n",
      "epoch: 2 training batch risk: 0.252 train risk: 0.257 test risk: 0.410\n",
      "[1.11s] 22. iteration, 2. epoch\n",
      "epoch: 2 training batch risk: 0.261 train risk: 0.254 test risk: 0.415\n",
      "[1.18s] 23. iteration, 2. epoch\n",
      "epoch: 2 training batch risk: 0.252 train risk: 0.255 test risk: 0.422\n",
      "[1.25s] 24. iteration, 2. epoch\n",
      "epoch: 2 training batch risk: 0.255 train risk: 0.258 test risk: 0.429\n",
      "[1.31s] 25. iteration, 2. epoch\n",
      "epoch: 2 training batch risk: 0.250 train risk: 0.262 test risk: 0.436\n",
      "[1.37s] 26. iteration, 2. epoch\n",
      "epoch: 2 training batch risk: 0.240 train risk: 0.266 test risk: 0.441\n",
      "[1.40s] 27. iteration, 2. epoch\n",
      "epoch: 2 training batch risk: 0.283 train risk: 0.265 test risk: 0.440\n",
      "[1.46s] 28. iteration, 2. epoch\n",
      "epoch: 2 training batch risk: 0.276 train risk: 0.260 test risk: 0.433\n",
      "[1.53s] 29. iteration, 2. epoch\n",
      "epoch: 2 training batch risk: 0.282 train risk: 0.255 test risk: 0.425\n",
      "[1.59s] 30. iteration, 3. epoch\n",
      "epoch: 3 training batch risk: 0.249 train risk: 0.253 test risk: 0.419\n",
      "[1.62s] 31. iteration, 3. epoch\n",
      "epoch: 3 training batch risk: 0.229 train risk: 0.252 test risk: 0.413\n",
      "[1.69s] 32. iteration, 3. epoch\n",
      "epoch: 3 training batch risk: 0.258 train risk: 0.253 test risk: 0.410\n",
      "[1.76s] 33. iteration, 3. epoch\n",
      "epoch: 3 training batch risk: 0.248 train risk: 0.255 test risk: 0.408\n",
      "[1.80s] 34. iteration, 3. epoch\n",
      "epoch: 3 training batch risk: 0.246 train risk: 0.256 test risk: 0.407\n",
      "[1.95s] 35. iteration, 3. epoch\n",
      "epoch: 3 training batch risk: 0.277 train risk: 0.255 test risk: 0.407\n",
      "[2.02s] 36. iteration, 3. epoch\n",
      "epoch: 3 training batch risk: 0.245 train risk: 0.255 test risk: 0.408\n",
      "[2.09s] 37. iteration, 3. epoch\n",
      "epoch: 3 training batch risk: 0.265 train risk: 0.253 test risk: 0.409\n",
      "[2.13s] 38. iteration, 3. epoch\n",
      "epoch: 3 training batch risk: 0.265 train risk: 0.252 test risk: 0.411\n",
      "[2.16s] 39. iteration, 3. epoch\n",
      "epoch: 3 training batch risk: 0.255 train risk: 0.251 test risk: 0.413\n",
      "[2.24s] 40. iteration, 4. epoch\n",
      "epoch: 4 training batch risk: 0.237 train risk: 0.251 test risk: 0.416\n",
      "[2.31s] 41. iteration, 4. epoch\n",
      "epoch: 4 training batch risk: 0.256 train risk: 0.251 test risk: 0.417\n",
      "[2.34s] 42. iteration, 4. epoch\n",
      "epoch: 4 training batch risk: 0.254 train risk: 0.252 test risk: 0.418\n",
      "[2.36s] 43. iteration, 4. epoch\n",
      "epoch: 4 training batch risk: 0.249 train risk: 0.252 test risk: 0.419\n",
      "[2.40s] 44. iteration, 4. epoch\n",
      "epoch: 4 training batch risk: 0.257 train risk: 0.251 test risk: 0.418\n",
      "[2.42s] 45. iteration, 4. epoch\n",
      "epoch: 4 training batch risk: 0.268 train risk: 0.251 test risk: 0.416\n",
      "[2.44s] 46. iteration, 4. epoch\n",
      "epoch: 4 training batch risk: 0.225 train risk: 0.251 test risk: 0.415\n",
      "[2.46s] 47. iteration, 4. epoch\n",
      "epoch: 4 training batch risk: 0.269 train risk: 0.251 test risk: 0.414\n",
      "[2.50s] 48. iteration, 4. epoch\n",
      "epoch: 4 training batch risk: 0.234 train risk: 0.250 test risk: 0.413\n",
      "[2.52s] 49. iteration, 4. epoch\n",
      "epoch: 4 training batch risk: 0.265 train risk: 0.251 test risk: 0.413\n",
      "[2.54s] 50. iteration, 5. epoch\n",
      "epoch: 5 training batch risk: 0.243 train risk: 0.250 test risk: 0.413\n",
      "[2.56s] 51. iteration, 5. epoch\n",
      "epoch: 5 training batch risk: 0.257 train risk: 0.250 test risk: 0.413\n",
      "[2.60s] 52. iteration, 5. epoch\n",
      "epoch: 5 training batch risk: 0.275 train risk: 0.250 test risk: 0.413\n",
      "[2.63s] 53. iteration, 5. epoch\n",
      "epoch: 5 training batch risk: 0.258 train risk: 0.250 test risk: 0.413\n",
      "[2.65s] 54. iteration, 5. epoch\n",
      "epoch: 5 training batch risk: 0.230 train risk: 0.250 test risk: 0.413\n",
      "[2.69s] 55. iteration, 5. epoch\n",
      "epoch: 5 training batch risk: 0.233 train risk: 0.250 test risk: 0.413\n",
      "[2.71s] 56. iteration, 5. epoch\n",
      "epoch: 5 training batch risk: 0.252 train risk: 0.250 test risk: 0.413\n",
      "[2.74s] 57. iteration, 5. epoch\n",
      "epoch: 5 training batch risk: 0.253 train risk: 0.250 test risk: 0.414\n",
      "[2.78s] 58. iteration, 5. epoch\n",
      "epoch: 5 training batch risk: 0.256 train risk: 0.250 test risk: 0.414\n",
      "[2.80s] 59. iteration, 5. epoch\n",
      "epoch: 5 training batch risk: 0.243 train risk: 0.250 test risk: 0.414\n",
      "[2.82s] 60. iteration, 6. epoch\n",
      "epoch: 6 training batch risk: 0.218 train risk: 0.250 test risk: 0.415\n",
      "[2.86s] 61. iteration, 6. epoch\n",
      "epoch: 6 training batch risk: 0.258 train risk: 0.250 test risk: 0.415\n",
      "[2.89s] 62. iteration, 6. epoch\n",
      "epoch: 6 training batch risk: 0.273 train risk: 0.250 test risk: 0.415\n",
      "[2.91s] 63. iteration, 6. epoch\n",
      "epoch: 6 training batch risk: 0.249 train risk: 0.250 test risk: 0.414\n",
      "[2.95s] 64. iteration, 6. epoch\n",
      "epoch: 6 training batch risk: 0.270 train risk: 0.249 test risk: 0.414\n",
      "[2.97s] 65. iteration, 6. epoch\n",
      "epoch: 6 training batch risk: 0.253 train risk: 0.249 test risk: 0.412\n",
      "[2.99s] 66. iteration, 6. epoch\n",
      "epoch: 6 training batch risk: 0.254 train risk: 0.249 test risk: 0.412\n",
      "[3.03s] 67. iteration, 6. epoch\n",
      "epoch: 6 training batch risk: 0.243 train risk: 0.249 test risk: 0.412\n",
      "[3.06s] 68. iteration, 6. epoch\n",
      "epoch: 6 training batch risk: 0.228 train risk: 0.249 test risk: 0.412\n",
      "[3.08s] 69. iteration, 6. epoch\n",
      "epoch: 6 training batch risk: 0.251 train risk: 0.249 test risk: 0.412\n",
      "[3.14s] 70. iteration, 7. epoch\n",
      "epoch: 7 training batch risk: 0.243 train risk: 0.249 test risk: 0.412\n",
      "[3.24s] 71. iteration, 7. epoch\n",
      "epoch: 7 training batch risk: 0.247 train risk: 0.249 test risk: 0.412\n",
      "[3.33s] 72. iteration, 7. epoch\n",
      "epoch: 7 training batch risk: 0.256 train risk: 0.249 test risk: 0.413\n",
      "[3.35s] 73. iteration, 7. epoch\n",
      "epoch: 7 training batch risk: 0.254 train risk: 0.249 test risk: 0.413\n",
      "[3.38s] 74. iteration, 7. epoch\n",
      "epoch: 7 training batch risk: 0.277 train risk: 0.249 test risk: 0.413\n",
      "[3.41s] 75. iteration, 7. epoch\n",
      "epoch: 7 training batch risk: 0.243 train risk: 0.249 test risk: 0.413\n",
      "[3.43s] 76. iteration, 7. epoch\n",
      "epoch: 7 training batch risk: 0.244 train risk: 0.249 test risk: 0.413\n",
      "[3.45s] 77. iteration, 7. epoch\n",
      "epoch: 7 training batch risk: 0.232 train risk: 0.249 test risk: 0.414\n",
      "[3.49s] 78. iteration, 7. epoch\n",
      "epoch: 7 training batch risk: 0.251 train risk: 0.249 test risk: 0.413\n",
      "[3.51s] 79. iteration, 7. epoch\n",
      "epoch: 7 training batch risk: 0.238 train risk: 0.249 test risk: 0.413\n",
      "[3.53s] 80. iteration, 8. epoch\n",
      "epoch: 8 training batch risk: 0.240 train risk: 0.248 test risk: 0.413\n",
      "[3.55s] 81. iteration, 8. epoch\n",
      "epoch: 8 training batch risk: 0.247 train risk: 0.248 test risk: 0.414\n",
      "[3.59s] 82. iteration, 8. epoch\n",
      "epoch: 8 training batch risk: 0.235 train risk: 0.248 test risk: 0.415\n",
      "[3.61s] 83. iteration, 8. epoch\n",
      "epoch: 8 training batch risk: 0.239 train risk: 0.248 test risk: 0.415\n",
      "[3.64s] 84. iteration, 8. epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 training batch risk: 0.254 train risk: 0.248 test risk: 0.414\n",
      "[3.66s] 85. iteration, 8. epoch\n",
      "epoch: 8 training batch risk: 0.265 train risk: 0.248 test risk: 0.414\n",
      "[3.70s] 86. iteration, 8. epoch\n",
      "epoch: 8 training batch risk: 0.264 train risk: 0.248 test risk: 0.413\n",
      "[3.72s] 87. iteration, 8. epoch\n",
      "epoch: 8 training batch risk: 0.241 train risk: 0.248 test risk: 0.412\n",
      "[3.74s] 88. iteration, 8. epoch\n",
      "epoch: 8 training batch risk: 0.260 train risk: 0.248 test risk: 0.410\n",
      "[3.77s] 89. iteration, 8. epoch\n",
      "epoch: 8 training batch risk: 0.238 train risk: 0.249 test risk: 0.409\n",
      "[3.80s] 90. iteration, 9. epoch\n",
      "epoch: 9 training batch risk: 0.229 train risk: 0.249 test risk: 0.408\n",
      "[3.86s] 91. iteration, 9. epoch\n",
      "epoch: 9 training batch risk: 0.234 train risk: 0.249 test risk: 0.408\n",
      "[3.88s] 92. iteration, 9. epoch\n",
      "epoch: 9 training batch risk: 0.263 train risk: 0.248 test risk: 0.409\n",
      "[3.90s] 93. iteration, 9. epoch\n",
      "epoch: 9 training batch risk: 0.255 train risk: 0.248 test risk: 0.411\n",
      "[3.94s] 94. iteration, 9. epoch\n",
      "epoch: 9 training batch risk: 0.245 train risk: 0.248 test risk: 0.414\n",
      "[3.96s] 95. iteration, 9. epoch\n",
      "epoch: 9 training batch risk: 0.259 train risk: 0.248 test risk: 0.417\n",
      "[3.98s] 96. iteration, 9. epoch\n",
      "epoch: 9 training batch risk: 0.254 train risk: 0.249 test risk: 0.419\n",
      "[4.01s] 97. iteration, 9. epoch\n",
      "epoch: 9 training batch risk: 0.245 train risk: 0.249 test risk: 0.418\n",
      "[4.04s] 98. iteration, 9. epoch\n",
      "epoch: 9 training batch risk: 0.258 train risk: 0.248 test risk: 0.415\n",
      "[4.06s] 99. iteration, 9. epoch\n",
      "epoch: 9 training batch risk: 0.247 train risk: 0.248 test risk: 0.412\n",
      "[10.37s] 1000. iteration, 100. epoch\n",
      "epoch: 100 training batch risk: 0.241 train risk: 0.242 test risk: 0.410\n",
      "[17.13s] 2000. iteration, 200. epoch\n",
      "epoch: 200 training batch risk: 0.253 train risk: 0.242 test risk: 0.410\n",
      "[25.25s] 3000. iteration, 300. epoch\n",
      "epoch: 300 training batch risk: 0.237 train risk: 0.243 test risk: 0.408\n",
      "[33.07s] 4000. iteration, 400. epoch\n",
      "epoch: 400 training batch risk: 0.235 train risk: 0.243 test risk: 0.413\n",
      "[40.41s] 5000. iteration, 500. epoch\n",
      "epoch: 500 training batch risk: 0.243 train risk: 0.242 test risk: 0.413\n",
      "[47.91s] 6000. iteration, 600. epoch\n",
      "epoch: 600 training batch risk: 0.247 train risk: 0.242 test risk: 0.410\n",
      "[54.99s] 7000. iteration, 700. epoch\n",
      "epoch: 700 training batch risk: 0.241 train risk: 0.243 test risk: 0.408\n",
      "[63.41s] 8000. iteration, 800. epoch\n",
      "epoch: 800 training batch risk: 0.249 train risk: 0.242 test risk: 0.410\n",
      "[70.79s] 9000. iteration, 900. epoch\n",
      "epoch: 900 training batch risk: 0.211 train risk: 0.242 test risk: 0.408\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "N = Y.shape[0]\n",
    "for i in range(10000):\n",
    "    rows, epoch_no, sgd_scale = aux_optimization.yield_minibatch_rows(i, N, MINIBATCH)\n",
    "  \n",
    "    minibatch_TRAIN = TRAIN_MASK[rows,:].reshape(-1).astype(bool)\n",
    "  \n",
    "    minibatch_X = X[:,rows,:].reshape(X.shape[0], -1)\n",
    "    minibatch_X = minibatch_X[:,minibatch_TRAIN]\n",
    "  \n",
    "    minibatch_Y = Y[rows,:].reshape(-1)\n",
    "    minibatch_Y = minibatch_Y[minibatch_TRAIN]\n",
    "  \n",
    "    minibatch_predictions = decision_maker( totorch(minibatch_X.transpose()) ).view(-1)\n",
    "    minibatch_loss = loss(minibatch_predictions, totorch(minibatch_Y)).mean()\n",
    "    \n",
    "    minibatch_h = torch.masked_select(HSTAR_FROZEN[rows].view(-1), \n",
    "                                      torch.tensor(minibatch_TRAIN).type(env.ByteTensor))\n",
    "    minibatch_l = totorch(LAMBDA_TRAIN[rows].reshape(-1)[minibatch_TRAIN])\n",
    "    regularizer = ( minibatch_l * (minibatch_predictions-minibatch_h)**2 ).mean()\n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    (minibatch_loss+regularizer).backward()\n",
    "    #minibatch_loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "    if i%1000==0 or i<100:\n",
    "        print(\"[%.2fs] %i. iteration, %i. epoch\" % (time.time()-start, i, epoch_no))    \n",
    "        predictions = decision_maker(X_flat).view(-1)\n",
    "        print(\"epoch: %s training batch risk: %.3f train risk: %.3f test risk: %.3f\" % \n",
    "          (epoch_no, minibatch_loss.item(), \n",
    "           empirical_risk1(predictions, Y, TRAIN_MASK),\n",
    "           empirical_risk1(predictions, Y, TEST_MASK)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal risk: train:0.244 test:0.4045\n"
     ]
    }
   ],
   "source": [
    "predictions = decision_maker(X_flat).view(-1)\n",
    "dm_train_risk = empirical_risk1(predictions, Y, TRAIN_MASK).item()\n",
    "dm_test_risk = empirical_risk1(predictions, Y, TEST_MASK).item()\n",
    "print(\"optimal risk: train:%.3f test:%.4f\" % (dm_train_risk, dm_test_risk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [[SEED, LOSS, TILTED_Q, REGULARIZATION, LAMBDA, S, NUM_QUANTILES,\n",
    "            vi_train_risk, vi_test_risk, q_train_risk, q_test_risk, dm_train_risk, dm_test_risk]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to PMF_evaluation_1_tilted_0.5_const_0.0_1000_20.csv\n"
     ]
    }
   ],
   "source": [
    "path = \"PMF_evaluation_%i_%s_%s_%s_%s_%s_%s.csv\" % (SEED, LOSS, TILTED_Q, REGULARIZATION, LAMBDA, S, NUM_QUANTILES)\n",
    "print(\"Saving to %s\" % path)\n",
    "pd.DataFrame(results).to_csv(path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PPCA-likelhood-lc",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
